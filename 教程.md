# 教程
> 非全原创, 只是整理了自己走的弯路而已

## 序言

> 此处在口嗨

首先, 要明确自己部署本地大模型的**目的与代价**.

**代价**很直白地说, 就是对你电脑性能的考研.

我一开始以为大模型很占用存储空间, 事实并非如此, 

现在已经出现了很多"小模型"了, 就非工作需求而言, 够用了 (比如这次的 小型文本翻译功能);

世上大模型有很多种, 他们各有千秋, 你的目的决定了你该选择什么样子的大模型,

有专职翻译的模型, 有专职演绎的模型, 有支持多国语言的模型......

总之, 如果目标不明确不专一, 就选择 **通用模型** 就好.

当然, 你完全可以 **部署多个模型**, 让他们在你的电脑上各司其职, 各补所长.

> 不多说了, 感觉自己不是很擅长聊天, 直接进入正题

# 宿主
> 各个大模型寄居的地方, 一个 **统一管理大模型** 的地方

目前主流的宿主有:
- Ollama      - `安装便捷, 更简洁, 更纯粹, 响应更快速`
- LM Studio  - `安装繁琐, 有原生前端界面, 推理能力更强`
> 我选择的是Ollama

## Ollama 下载安装
- [bilibili_玄离199_Ollama的安装](https://www.bilibili.com/video/BV1bU411Z7eo/?spm_id_from=333.337.search-card.all.click&vd_source=e4b7dd5eaa29901faf6abac93b353a81)
- [Ollama官网链接](https://ollama.com/)
> 跟着看就行了, 很好操作, 但别着急安装, 下面的内容先看了!!!
> Ollama默认安装在C盘, 这导致后续的大模型也会跟着安装在C盘 !!!

## Ollama 修改安装路径
- [自定义Ollama安装路径 - 但风偏偏，雨渐渐 - 博客园](https://www.cnblogs.com/LaiYun/p/18696931)
> 跟着里面教程看就行了

## Ollama 关闭开机自启动
- [如何取消OLLAMA的开机自启动以及更改默认下载到C盘的模型地址到其它地方_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV17hNgecEAp/?vd_source=e4b7dd5eaa29901faf6abac93b353a81)
> 看就行了

# 模型选择
> 本人配置为 `天选4笔记本 4060 8GB`

综合考量了 `社区环境, 电脑配置, 部署用途` 之后, 
我选择了 `DeepSeek R1 7b`
> 7b也就是70亿训练数据出来的模型, 已经是我显卡的运行极限了

# Ollama 端口暴露问题
简单来说, **Ollama的默认端口会暴露在公网**,   
导致该端口被其它人访问, 可能会被人偷算力啥的.  
最好的解决方法就是**修改Ollama的本地端口号.**  
**但是!!!**  
我整了很久都没法解决修改端口这么一个`简单的问题`  
Ollama一直我行我素, 不用我设置的端口运行, 一直都是默认端口  
我没办法了, 之后就去网罗资料,  
得知这个 **暴露问题** 在国内不太需要担心,  
因为国内的网络部署方式大多为`广内网, 少公网`  
而国外则相反, 他们好像很多有都有公网IP.
简单来说就是, 你在国内把自己暴露在公网时需要**申请**一个`公网IP`  
申请公网IP是一个很麻烦的事情, 默认广大同学都没搞过,   
所以**不用太担心**这个端口暴露问题,   
申请了公网IP的人, `他们自己知道他们在干什么`